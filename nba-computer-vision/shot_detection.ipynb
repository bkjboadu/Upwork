{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSTALL AND IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2,labelme\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os,uuid\n",
    "import matplotlib.pyplot as plt\n",
    "import time,json,wget\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np,pandas as pd\n",
    "import albumentations as A\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord_labelme.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {\n",
    "    'workspace':Path('workspace'),\n",
    "    'images':Path('workspace','images'),\n",
    "    'train-images':Path('workspace','images','train'),\n",
    "    'test-images':Path('workspace','images','test'),\n",
    "    'protoc': Path('workspace','protoc'),\n",
    "    'scripts': Path('workspace','scripts'),\n",
    "    'pretrained-model':Path('workspace','pretrained_model'),\n",
    "    'annotation':Path('workspace','annotation'),\n",
    "    'models': Path('workspace','models'),\n",
    "    'CHECKPOINT_PATH': Path('workspace','models',CUSTOM_MODEL_NAME),\n",
    "    'OUTPUT_PATH': Path('workspace','models',CUSTOM_MODEL_NAME,'export'),\n",
    "    'TFJS_PATH': Path('workspace','models',CUSTOM_MODEL_NAME,'tfjsexport'),\n",
    "    'TFLITE_PATH': Path('workspace','models',CUSTOM_MODEL_NAME,'tfliteexport'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'TF_RECORD_SCRIPTS': Path(PATHS['scripts'],TF_RECORD_SCRIPT_NAME),\n",
    "    'LABELMAP': Path(PATHS['annotation'],LABEL_MAP_NAME),\n",
    "    'PIPELINE_PATH':os.path.join(str(PATHS['pretrained-model']),'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8','pipeline.config'),\n",
    "    'CHECKPOINT_PATH': os.path.join(str(PATHS['pretrained-model']),'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8','checkpoint')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bkj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_PATH'])\n",
    "detection_model = model_builder.build(model_config=configs['model'],is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATHS['CHECKPOINT_PATH'],'ckpt-3')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image,shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image,shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict,shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])\n",
    "IMAGEFILE_PATH = os.path.join(PATHS['test-images'],'2b11bcd1-881e-11ee-bffd-ec5c68664d70.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def get_angels(results):\n",
    "    # Calculate the wrist\n",
    "        wrist = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "        index = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_INDEX]\n",
    "        elbow = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        \n",
    "        wrist = np.array([wrist.x, wrist.y, wrist.z])\n",
    "        index = np.array([index.x, index.y, index.z])\n",
    "        elbow = np.array([elbow.x, elbow.y, elbow.z])\n",
    "\n",
    "        v1 = index - wrist\n",
    "        v2 = elbow - wrist\n",
    "        wrist_angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "        wrist_angle = np.degrees(wrist_angle)\n",
    "        if index[1] < wrist[1]:\n",
    "          wrist_angle = -wrist_angle\n",
    "\n",
    "\n",
    "        # Calculate the right elbow angle\n",
    "        shoulder = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        elbow = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        wrist = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "        shoulder = np.array([shoulder.x, shoulder.y, shoulder.z])\n",
    "        elbow = np.array([elbow.x, elbow.y, elbow.z])\n",
    "        wrist = np.array([wrist.x, wrist.y, wrist.z])\n",
    "        v1 = shoulder - elbow\n",
    "        v2 = wrist - elbow\n",
    "        elbow_angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "        elbow_angle = np.degrees(elbow_angle)\n",
    "\n",
    "        # Calculate the right shoulder angle\n",
    "        hip = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        hip = np.array([hip.x, hip.y, hip.z])\n",
    "        v1 = elbow - shoulder\n",
    "        v2 = hip - shoulder\n",
    "        shoulder_angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "        shoulder_angle = np.degrees(shoulder_angle)\n",
    "        \n",
    "        # Calculate the hip angle\n",
    "        knee = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        knee = np.array([knee.x, knee.y, knee.z])\n",
    "        v1 = shoulder - hip\n",
    "        v2 = knee - hip\n",
    "        hip_angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "        hip_angle = np.degrees(hip_angle)\n",
    "\n",
    "        # Calculate the right knee angle\n",
    "        ankle = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "        ankle = np.array([ankle.x, ankle.y, ankle.z])\n",
    "        v1 = hip - knee\n",
    "        v2 = ankle - knee\n",
    "        knee_angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "        knee_angle = np.degrees(knee_angle)\n",
    "\n",
    "        # Calculate the right ankle angle\n",
    "        heel = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL]\n",
    "        heel = np.array([heel.x, heel.y, heel.z])\n",
    "        v1 = knee - ankle\n",
    "        v2 = heel - ankle\n",
    "        ankle_angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "        ankle_angle = np.degrees(ankle_angle)\n",
    "\n",
    "        return [wrist_angle,elbow_angle,shoulder_angle,hip_angle,knee_angle,ankle_angle]\n",
    "\n",
    "\n",
    "def display_angle_table(frame,results):\n",
    "    \n",
    "    wrist_angle,elbow_angle,shoulder_angle,hip_angle,knee_angle,ankle_angle = get_angels(results)\n",
    "\n",
    "    cv2.rectangle(frame, (width - 600, height - 250), (width, height), (0, 0, 0), cv2.FILLED)\n",
    "    cv2.line(frame,(width - 600, height - 250), (width - 600, height ), (255,255,255), 4)\n",
    "    for i in [250,210,170,130,90,50]:\n",
    "        cv2.line(frame, (width - 600, height - i), (width, height - i), (255,255,255), 4)\n",
    "    cv2.putText(frame, f'Right wrist angle: {wrist_angle:.2f} degs', (width - 600, height - 220), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Right elbow angle: {elbow_angle:.2f} degs', (width - 600, height - 180), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Right shoulder angle: {shoulder_angle:.2f} degs', (width - 600, height - 140), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Hip angle: {hip_angle:.2f} degs', (width - 600, height - 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Right knee angle: {knee_angle:.2f} degs', (width - 600, height - 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Right ankle angle: {ankle_angle:.2f} degs', (width - 600, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    results = model.process(image)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return results\n",
    "\n",
    "def draw_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(10, 250, 80), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(250, 0, 0), thickness=2, circle_radius=2),)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(0, 250, 80), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(250, 0, 250), thickness=2, circle_radius=2),)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(10, 250, 80), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(121, 121, 255), thickness=2, circle_radius=2),)\n",
    "    \n",
    "\n",
    "def extract_keypoint(results):\n",
    "    face = np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    pose = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,face,lh,rh])\n",
    "\n",
    "def extract_images(video_path,output_folder,frames_to_extract):\n",
    "    '''\n",
    "    extracts images from video\n",
    "    \n",
    "    Arguments: \n",
    "        video_path: this is the path to the video we are extracting image from\n",
    "        output_file: this is the directory our extracted image will be saved to\n",
    "        frame_interval_seconds: this is the seconds interval to save each image\n",
    "    '''\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    frame_on_save = 0\n",
    "\n",
    "    Path(output_folder).mkdir(exist_ok=True)\n",
    "    folder_name = Path(video_path).stem\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        # current_time_seconds = frame_count / cap.get(cv2.CAP_PROP_FPS)\n",
    "        # print(current_time_seconds)\n",
    "\n",
    "        # Save the frame if it's within the desired interval\n",
    "        if  frame_count in frames_to_extract:\n",
    "\n",
    "            final_npy_dir = Path(output_folder,folder_name,f\"{frame_on_save}.npy\")\n",
    "            image_np = np.array(frame)\n",
    "        \n",
    "            input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "            detections = detect_fn(input_tensor)\n",
    "            \n",
    "            num_detections = int(detections.pop('num_detections'))\n",
    "            detections = {key: value[0, :num_detections].numpy()\n",
    "                            for key, value in detections.items()}\n",
    "            detections['num_detections'] = num_detections\n",
    "\n",
    "            # detection_classes should be ints.\n",
    "            detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "            label_id_offset = 1\n",
    "            image_np_with_detections = image_np.copy()\n",
    "\n",
    "            # for i in range(len(detections['detection_boxes'])):\n",
    "            box = detections['detection_boxes'][0]\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            ymin, xmin, ymax, xmax = int(ymin * height), int(xmin * width), int(ymax * height), int(xmax * width)\n",
    "\n",
    "            player_frame = image_np[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            # image,results = mediapipe_detection(player_frame,holistic)\n",
    "\n",
    "            # print(results)\n",
    "\n",
    "            # # results_np = extract_keypoint(results)\n",
    "            # # np.save(results_np,final_npy_dir)\n",
    "            # # frame_to_save += 1\n",
    "\n",
    "            output_path = Path(output_folder, f\"{str(uuid.uuid1())}.jpg\")\n",
    "            print(output_path)\n",
    "            cv2.imwrite(str(output_path), player_frame)\n",
    "    cap.release()\n",
    "\n",
    "def view_fn(path,starting_point):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frame_count = 0\n",
    "    frame_to_save = 0\n",
    "    folder_name = Path(path).stem\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret,frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            image_np = np.array(frame)\n",
    "            \n",
    "            input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "            detections = detect_fn(input_tensor)\n",
    "            \n",
    "            num_detections = int(detections.pop('num_detections'))\n",
    "            detections = {key: value[0, :num_detections].numpy()\n",
    "                            for key, value in detections.items()}\n",
    "            detections['num_detections'] = num_detections\n",
    "\n",
    "            # detection_classes should be ints.\n",
    "            detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "            label_id_offset = 1\n",
    "            image_np_with_detections = image_np.copy()\n",
    "\n",
    "            # for i in range(len(detections['detection_boxes'])):\n",
    "            box = detections['detection_boxes'][0]\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            ymin, xmin, ymax, xmax = int(ymin * height), int(xmin * width), int(ymax * height), int(xmax * width)\n",
    "\n",
    "            player_frame = image_np[ymin:ymax, xmin:xmax]\n",
    "            \n",
    "            margin = 50\n",
    "\n",
    "            text_x = max(min(xmin - margin, player_frame.shape[0] - 150), 0)\n",
    "            text_y = max(min(ymin - margin, player_frame.shape[1] - 150), 0)\n",
    "\n",
    "            \n",
    "            if text_x < player_frame.shape[1] and text_y < player_frame.shape[0]:\n",
    "                cv2.putText(player_frame, f'{frame_count}', (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                print(\"Text coordinates exceed frame dimensions.\")\n",
    "            cv2.imshow('frame',player_frame)\n",
    "            \n",
    "\n",
    "            if frame_count in np.arange(starting_point,starting_point + 25):\n",
    "                image,results = mediapipe_detection(player_frame,holistic)\n",
    "                extracted_results = extract_keypoint(results)\n",
    "                final_npy_dir = Path('shot_detection_images','0',folder_name,f\"{frame_to_save}.npy\")\n",
    "                Path('shot_detection_images','0',folder_name).mkdir(exist_ok=True)\n",
    "                np.save(final_npy_dir,extracted_results)\n",
    "                draw_landmarks(image,results)\n",
    "                mp_drawing.draw_landmarks(player_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(10, 250, 80), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(250, 0, 0), thickness=2, circle_radius=2),)\n",
    "                frame_to_save += 1\n",
    "            elif frame_count > starting_point + 25:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture('videos/JordanWalker5Shots.MOV')\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = 0\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "shot_num = 0\n",
    "with mp_pose.Pose() as pose:\n",
    "  while cap.isOpened(): \n",
    "      ret, frame = cap.read()\n",
    "\n",
    "      frame_count += 1\n",
    "      image_np = np.array(frame)\n",
    "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "      # Indicate frame number and fps at the top-left corner\n",
    "      cv2.putText(image_np, f'Frame: {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "      cv2.putText(image_np, f'FPS: {fps:.2f}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "      cv2.putText(image_np, f'Shot number: {shot_num}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "      \n",
    "      input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "      detections = detect_fn(input_tensor)\n",
    "      num_detections = int(detections.pop('num_detections'))\n",
    "      detections = {key: value[0, :num_detections].numpy()\n",
    "                    for key, value in detections.items()}\n",
    "      detections['num_detections'] = num_detections\n",
    "\n",
    "      # detection_classes should be ints.\n",
    "      detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "      label_id_offset = 1\n",
    "      image_np_with_detections = image_np.copy()\n",
    "\n",
    "      box = detections['detection_boxes'][0]\n",
    "      ymin, xmin, ymax, xmax = box\n",
    "      ymin, xmin, ymax, xmax = int(ymin * height), int(xmin * width), int(ymax * height), int(xmax * width)\n",
    "\n",
    "      cv2.rectangle(image_np_with_detections, (xmin, ymin), (xmax, ymax), (0, 255, 0), 4)\n",
    "\n",
    "      # Crop the player region and extract mediapip keypoints\n",
    "      player_frame = image_np[ymin:ymax, xmin:xmax]\n",
    "      results = mediapipe_detection(player_frame,holistic)\n",
    "      keypoints = extract_keypoint(results)\n",
    "  \n",
    "      # draw landmarks on keypoints and display angles on screen\n",
    "      draw_landmarks(player_frame,results)\n",
    "      if results.pose_landmarks:\n",
    "         display_angle_table(image_np_with_detections,results)\n",
    "\n",
    "      image_np_with_detections[ymin:ymax, xmin:xmax] = player_frame\n",
    "      cv2.imshow('object detection', cv2.resize(image_np_with_detections, (1200, 600)))\n",
    "\n",
    "      \n",
    "      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "          cap.release()\n",
    "          cv2.destroyAllWindows()\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_details(path,frame_extract,shot_status='shot'):\n",
    "    frame_extract = combine_frame_list(frame_extract)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    folder_status = 'shot' if shot_status == 'shot' else 'noshot'\n",
    "    folder_name = Path('shot_images')/ f\"{folder_status}\" / Path(path).stem\n",
    "    folder_numpy = Path('shot_images')/ f\"{folder_status}_numpy\" / Path(path).stem\n",
    "\n",
    "    ravel_frame_extract = np.array(frame_extract).ravel()\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = 0\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    shot_num = 0\n",
    "    with mp_pose.Pose() as pose:\n",
    "        while cap.isOpened(): \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count > ravel_frame_extract[-1]:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            image_np = np.array(frame)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "            # Indicate frame number and fps at the top-left corner\n",
    "            cv2.putText(image_np, f'Frame: {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image_np, f'FPS: {fps:.2f}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image_np, f'Shot number: {shot_num}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "            input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "            detections = detect_fn(input_tensor)\n",
    "            num_detections = int(detections.pop('num_detections'))\n",
    "            detections = {key: value[0, :num_detections].numpy()\n",
    "                            for key, value in detections.items()}\n",
    "            detections['num_detections'] = num_detections\n",
    "\n",
    "            # detection_classes should be ints.\n",
    "            detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "            label_id_offset = 1\n",
    "            image_np_with_detections = image_np.copy()\n",
    "\n",
    "            box = detections['detection_boxes'][0]\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            ymin, xmin, ymax, xmax = int(ymin * height), int(xmin * width), int(ymax * height), int(xmax * width)\n",
    "\n",
    "            cv2.rectangle(image_np_with_detections, (xmin, ymin), (xmax, ymax), (0, 255, 0), 4)\n",
    "\n",
    "            # Crop the player region and extract mediapip keypoints\n",
    "            player_frame = image_np[ymin:ymax, xmin:xmax]\n",
    "            results = mediapipe_detection(player_frame,holistic)\n",
    "            keypoints = extract_keypoint(results)\n",
    "            \n",
    "            if frame_count in ravel_frame_extract:\n",
    "                vid_sequence = 0\n",
    "                for sequence in frame_extract:\n",
    "                    if frame_count in sequence:\n",
    "                        shot_dir = Path(folder_name) / f\"{vid_sequence}\"\n",
    "                        shot_dir.mkdir(parents=True,exist_ok=True)\n",
    "                        shot_numpy = Path(folder_numpy) / f\"{vid_sequence}\"\n",
    "                        shot_numpy.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "                        cv2.imwrite(str(Path(shot_dir)/f\"{frame_count}.jpg\"),player_frame)\n",
    "                        np.save(str(Path(shot_numpy)/f\"{frame_count}.npy\"),keypoints)\n",
    "                    vid_sequence += 1\n",
    "            \n",
    "        \n",
    "            # draw landmarks on keypoints and display angles on screen\n",
    "            draw_landmarks(player_frame,results)\n",
    "            if results.pose_landmarks:\n",
    "                display_angle_table(image_np_with_detections,results)\n",
    "\n",
    "            image_np_with_detections[ymin:ymax, xmin:xmax] = player_frame\n",
    "            cv2.imshow('object detection', cv2.resize(image_np_with_detections, (1200, 600)))\n",
    "\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "def combine_frame_list(items):\n",
    "    x = []\n",
    "    for item in items:\n",
    "        item_arange = np.arange(item,item+5).tolist()\n",
    "        x.append(item_arange)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_frame_details('videos/CharlieBrownDelaware5Shots.mov',[170,620,1114,1627,2142])\n",
    "# get_frame_details('videos/CoreyHawkins5Shots.mov',[92,400,714,997,1268])\n",
    "# get_frame_details('videos/DavonteFitzgerald5Shots.mov',[248,564,906,1192,1495])\n",
    "# get_frame_details('videos/Elfrid5Shots.mov',[125,446,781,1208,1640])\n",
    "# get_frame_details('videos/Facu5shots.mov',[120,391,665,998,1315])\n",
    "# get_frame_details('videos/GregBrown5Shots.mov',[195,627,1076,1592,2124])\n",
    "# get_frame_details('videos/JaredBrownrigdge5shots.mov',[188,803,1281,1752,2263])\n",
    "# get_frame_details('videos/JordanWalker5Shots.MOV',[150,579,1045,1462,1890])\n",
    "# get_frame_details('videos/KylorKelley5Shots.mov',[220,702,1179,1826,2327])\n",
    "# get_frame_details('videos/Maxi5Shot.mov',[165,571,1115,1608,2070])\n",
    "# get_frame_details('videos/OMax5Shot.mov',[55,634,1100,1558,2108])\n",
    "# get_frame_details('videos/SergeIbaka x5.mov',[166,449,718,1014,1381])\n",
    "# get_frame_details('videos/SteveSir5Shots.mov',[170,605,1144,1663,2102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_frame_details('videos/CharlieBrownDelaware5Shots.mov',[1,90,240,290,350],shot_status='noshot')\n",
    "# get_frame_details('videos/CoreyHawkins5Shots.mov',[1,58,142,160,180],shot_status='noshot')\n",
    "# get_frame_details('videos/DavonteFitzgerald5Shots.mov',[1,45,95,210,290],shot_status='noshot')\n",
    "# get_frame_details('videos/Elfrid5Shots.mov',[1,85,165,180,205],shot_status='noshot')\n",
    "# get_frame_details('videos/Facu5shots.mov',[1,80,170,235,270],shot_status='noshot')\n",
    "# get_frame_details('videos/GregBrown5Shots.mov',[1,70,155,300,337],shot_status='noshot')\n",
    "# get_frame_details('videos/JaredBrownrigdge5shots.mov',[1,100,135,265,320],shot_status='noshot')\n",
    "# get_frame_details('videos/JordanWalker5Shots.MOV',[1,70,110,230,280],shot_status='noshot')\n",
    "# get_frame_details('videos/KylorKelley5Shots.mov',[1,160,280,310,340],shot_status='noshot')\n",
    "# get_frame_details('videos/Maxi5Shot.mov',[1,75,115,260,320],shot_status='noshot')\n",
    "# get_frame_details('videos/OMax5Shot.mov',[1,15,200,260,325],shot_status='noshot')\n",
    "# get_frame_details('videos/SergeIbaka x5.mov',[1,15,130,270,300],shot_status='noshot')\n",
    "# get_frame_details('videos/SteveSir5Shots.mov',[1,160,280,310,340],shot_status='noshot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET SEQUENCE AND LABELS FOR MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noshot': 0, 'shot': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = ['noshot','shot']\n",
    "label_map = {label:num for num,label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels = [],[]\n",
    "for action in actions:\n",
    "    action_path = Path('shot_images') / f\"{action}_numpy\"\n",
    "    action_path_dir = [x for x in action_path.glob('*')]\n",
    "    for player_dir in action_path_dir:\n",
    "        player_vid_sequence = [x for x in player_dir.glob('*')]\n",
    "        for vid_sequence in player_vid_sequence:\n",
    "            files = [np.load(x) for x in vid_sequence.glob('*.npy')]\n",
    "            sequences.append(files)\n",
    "            labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.05345559e-01,  1.56246677e-01,  1.52761608e-01, ...,\n",
       "         5.05054355e-01,  3.47891092e-01, -1.91611107e-04]),\n",
       " array([ 0.41736901,  0.14443187, -0.08474232, ...,  0.49452993,\n",
       "         0.34036329, -0.00490887]),\n",
       " array([ 0.42430997,  0.14329419, -0.11073017, ...,  0.51390576,\n",
       "         0.33851627,  0.00606952]),\n",
       " array([ 0.42514628,  0.1421883 , -0.01147161, ...,  0.51562226,\n",
       "         0.34155437, -0.00117965]),\n",
       " array([ 0.42122105,  0.14229096,  0.00377664, ...,  0.51022702,\n",
       "         0.33731213, -0.00425645])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noshot\n",
      "shot\n"
     ]
    }
   ],
   "source": [
    "sequences,labels = [],[]\n",
    "for action in actions:\n",
    "    print(action)\n",
    "    action_path = Path('shot_detection_images',action)\n",
    "    action_files = [x for x in action_path.glob('*')]\n",
    "    for player_file in action_files:\n",
    "        window = []\n",
    "        if not player_file.is_dir():\n",
    "            continue\n",
    "        for i in range(25):\n",
    "            image_array = np.load(str(Path(player_file,f\"{i}.npy\")))\n",
    "            window.append(image_array)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 5, 1662)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILD AND TRAIN LSTM NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5, 64)             442112    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5, 128)            98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,642\n",
      "Trainable params: 596,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(5,1662)))\n",
    "model_lstm.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model_lstm.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "model_lstm.add(Dense(64,activation='relu'))\n",
    "model_lstm.add(Dense(32,activation='relu'))\n",
    "model_lstm.add(Dense(2,activation='softmax'))\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 [==============================] - 19s 52ms/step - loss: 0.6854 - accuracy: 0.7212\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.5842 - accuracy: 0.7500\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4738 - accuracy: 0.7885\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4040 - accuracy: 0.8269\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3141 - accuracy: 0.8654\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2887 - accuracy: 0.8750\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2289 - accuracy: 0.8942\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2252 - accuracy: 0.8942\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.1835 - accuracy: 0.9038\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.1286 - accuracy: 0.9423\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.1109 - accuracy: 0.9615\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0974 - accuracy: 0.9615\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0607 - accuracy: 0.9712\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0877 - accuracy: 0.9712\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0936 - accuracy: 0.9712\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.0686 - accuracy: 0.8654\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.0475 - accuracy: 0.7981\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2254 - accuracy: 0.9038\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.2273 - accuracy: 0.8846\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.2031 - accuracy: 0.8846\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.1706 - accuracy: 0.9327\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.1724 - accuracy: 0.9519\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.1748 - accuracy: 0.9519\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.1426 - accuracy: 0.9519\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.1117 - accuracy: 0.9519\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.1339 - accuracy: 0.9231\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0994 - accuracy: 0.9615\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0890 - accuracy: 0.9615\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0814 - accuracy: 0.9615\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0675 - accuracy: 0.9712\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0556 - accuracy: 0.9712\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0404 - accuracy: 0.9808\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0317 - accuracy: 0.9904\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0230 - accuracy: 0.9904\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0234 - accuracy: 0.9904\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3038 - accuracy: 0.9904\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0164 - accuracy: 0.9904\n",
      "Epoch 38/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.0194 - accuracy: 0.9904\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 8.8789e-04 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 6.3940e-04 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 5.0769e-04 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 2.8846e-04 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.2674e-04 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.7485e-04 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.3852e-04 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.0191e-04 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.1266e-04 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.0790e-04 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 8.3910e-05 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 4.7517e-05 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 5.0432e-05 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 5.3501e-05 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 4.8965e-05 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 4.3224e-05 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 3.0928e-05 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.6295e-05 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 2.3991e-05 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.2285e-05 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.0450e-05 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.9251e-05 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.7778e-05 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.6565e-05 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 1.5551e-05 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.4767e-05 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.3767e-05 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.3130e-05 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.2391e-05 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1854e-05 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.1099e-05 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.0387e-05 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 9.8896e-06 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 9.3459e-06 - accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 8.7061e-06 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 8.4581e-06 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 8.1476e-06 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 7.6234e-06 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 7.1634e-06 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 6.6951e-06 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 6.3628e-06 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 5.9782e-06 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 5.6240e-06 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 5.3507e-06 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 5.1030e-06 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 4.8936e-06 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 4.7083e-06 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 4.5172e-06 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 4.3264e-06 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 4.2124e-06 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 4.0102e-06 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 3.8407e-06 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 3.6733e-06 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.5910e-06 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 3.4296e-06 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 3.2583e-06 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.1539e-06 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 3.0520e-06 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.9604e-06 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.9508e-06 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.6245e-06 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.4355e-06 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 2.4323e-06 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.3155e-06 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 2.2514e-06 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.1756e-06 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.1033e-06 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.0537e-06 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.9298e-06 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.8274e-06 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.7557e-06 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.6840e-06 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.6271e-06 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.6075e-06 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 1.5678e-06 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.5350e-06 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.5059e-06 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.4686e-06 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.4224e-06 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.3713e-06 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 1.3365e-06 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.2955e-06 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.2752e-06 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 1.2516e-06 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.2070e-06 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.1712e-06 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.1407e-06 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.1084e-06 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.0733e-06 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.0599e-06 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.0215e-06 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 9.9426e-07 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 9.9632e-07 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 9.2514e-07 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 9.4564e-07 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 9.3385e-07 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 8.8153e-07 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 8.3415e-07 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 7.9643e-07 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 7.6513e-07 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 7.7372e-07 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 7.6014e-07 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 7.5579e-07 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251cbc32800>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(x_train,y_train,epochs=150,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 5, 1662)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_lstm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hat = np.argmax(y_pred,axis=1)\n",
    "y_test_hat = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846153846153846"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_hat,y_pred_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save('shot_detection_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
